Linear Models
================
wz2631
2022-12-03

### Problem 1

To obtain a distribution for $\hat{r}^2$, we’ll follow basically the
same procedure we used for regression coefficients: draw bootstrap
samples; the a model to each; extract the value I’m concerned with; and
summarize. Here, we’ll use `modelr::bootstrap` to draw the samples and
`broom::glance` to produce `r.squared` values.

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

    ## Registered S3 method overwritten by 'hoardr':
    ##   method           from
    ##   print.cache_info httr

    ## using cached file: ~/Library/Caches/R/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2022-12-02 21:48:31 (8.426)

    ## file min/max dates: 1869-01-01 / 2022-11-30

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

<img src="p8105_hw6_wz2631_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1
may be a cause for the generally skewed shape of the distribution. If we
wanted to construct a confidence interval for $R^2$, we could take the
2.5% and 97.5% quantiles of the estimates across bootstrap samples.
However, because the shape isn’t symmetric, using the mean +/- 1.96
times the standard error probably wouldn’t work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a
similar approach, with a bit more wrangling before we make our plot.

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

<img src="p8105_hw6_wz2631_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

As with $r^2$, this distribution is somewhat skewed and has some
outliers.

The point of this is not to say you should always use the bootstrap –
it’s possible to establish “large sample” distributions for strange
parameters / values / summaries in a lot of cases, and those are great
to have. But it is helpful to know that there’s a way to do inference
even in tough cases.

### Problem 2

#### Import and tidy data of data on homicides in 50 large U.S. cities.

I create a city_state variable and a binary variable indicating whether
the homicide is solved. I omit cities Dallas, TX; Phoenix, AZ; Kansas
City, MO and Tulsa, AL. Besides, I limit my analysis those for whom
victim_race is white or black. The variable victim_age is numeric.

``` r
homicide_data = 
  read_csv("./rawdata/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, ', ',state),
    victim_age = as.numeric(victim_age),
    homicide_statu =  ifelse(disposition != "Closed by arrest", 0, 1)
    ) %>% 
  filter(city_state != "Dallas, TX",
         city_state != "Phoenix, AZ",
         city_state != "Kansas City, MO",
         city_state != "Tulsa, AL") %>% 
  filter(victim_race == "White" | victim_race == "Black") %>% 
  drop_na()
```

#### Fit a logistic regression for the city of Baltimore, MD.

For the city of Baltimore, MD, I use the glm function to fit a logistic
regression with resolved vs unresolved as the outcome and victim age,
sex and race as predictors. Besides, I save the output of glm as an R
object; apply the broom::tidy to this object; and obtain the estimate
and confidence interval of the adjusted odds ratio for solving homicides
comparing male victims to female victims keeping all other variables
fixed.

``` r
baltimore_data = 
  homicide_data %>% 
  filter(city_state == "Baltimore, MD")
fit_bal_data = 
  glm(homicide_statu ~ victim_age + victim_sex + victim_race, data = baltimore_data, family = binomial) %>% 
  broom::tidy(conf.int = T, conf.level = 0.95) %>% 
  mutate(
    odds_ratio = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, odds_ratio, CI_lower, CI_upper) %>% 
  knitr::kable(digits = 2.5)
fit_bal_data
```

| term           | odds_ratio | CI_lower | CI_upper |
|:---------------|-----------:|---------:|---------:|
| victim_sexMale |      0.426 |    0.325 |    0.558 |

The estimate of the adjusted odds ratio for solving homicides comparing
male victims to female victims is 0.426, and the confidence interval of
which is (0.325,0.558). It can be concluded that keeping all other
variables fixed, there are lower chances for homicides to be solved when
victims are male compared with that when victims are female.

#### Fit a logistic regression for each city in the dataset.

Run glm for each of the cities in the dataset, and extract the adjusted
odds ratio (and CI) for solving homicides comparing male victims to
female victims. Do this within a “tidy” pipeline, making use of
purrr::map, list columns, and unnest as necessary to create a dataframe
with estimated ORs and CIs for each city.

``` r
fit_all_data =
  homicide_data %>% 
  select(city_state, homicide_statu, victim_race:victim_sex) %>% 
  nest(data = homicide_statu:victim_sex) %>% 
  mutate(
    fit = map(data, ~glm(homicide_statu ~ victim_age + victim_sex + victim_race, data =.x, family = binomial())),
   results = map(fit, broom::tidy)
   ) %>%
  unnest(results) %>%
  mutate(
    odds_ratio_all = exp(estimate),
    CI_lower_all = exp(estimate - 1.96 * std.error),
    CI_upper_all = exp(estimate + 1.96 * std.error)
  ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, city_state, odds_ratio_all, CI_lower_all, CI_upper_all) %>% 
  knitr::kable(digits = 2.5)
fit_all_data
```

| term           | city_state         | odds_ratio_all | CI_lower_all | CI_upper_all |
|:---------------|:-------------------|---------------:|-------------:|-------------:|
| victim_sexMale | Albuquerque, NM    |          1.734 |        0.814 |        3.695 |
| victim_sexMale | Atlanta, GA        |          1.000 |        0.684 |        1.463 |
| victim_sexMale | Baltimore, MD      |          0.426 |        0.325 |        0.558 |
| victim_sexMale | Baton Rouge, LA    |          0.381 |        0.209 |        0.695 |
| victim_sexMale | Birmingham, AL     |          0.873 |        0.576 |        1.322 |
| victim_sexMale | Boston, MA         |          0.672 |        0.355 |        1.272 |
| victim_sexMale | Buffalo, NY        |          0.522 |        0.291 |        0.939 |
| victim_sexMale | Charlotte, NC      |          0.884 |        0.557 |        1.403 |
| victim_sexMale | Chicago, IL        |          0.410 |        0.336 |        0.501 |
| victim_sexMale | Cincinnati, OH     |          0.400 |        0.236 |        0.677 |
| victim_sexMale | Columbus, OH       |          0.546 |        0.387 |        0.770 |
| victim_sexMale | Denver, CO         |          0.479 |        0.236 |        0.971 |
| victim_sexMale | Detroit, MI        |          0.582 |        0.462 |        0.734 |
| victim_sexMale | Durham, NC         |          0.812 |        0.392 |        1.683 |
| victim_sexMale | Fort Worth, TX     |          0.669 |        0.397 |        1.127 |
| victim_sexMale | Fresno, CA         |          1.224 |        0.522 |        2.873 |
| victim_sexMale | Houston, TX        |          0.705 |        0.553 |        0.899 |
| victim_sexMale | Indianapolis, IN   |          0.919 |        0.679 |        1.242 |
| victim_sexMale | Jacksonville, FL   |          0.720 |        0.537 |        0.966 |
| victim_sexMale | Las Vegas, NV      |          0.847 |        0.614 |        1.168 |
| victim_sexMale | Long Beach, CA     |          0.410 |        0.156 |        1.082 |
| victim_sexMale | Los Angeles, CA    |          0.662 |        0.458 |        0.956 |
| victim_sexMale | Louisville, KY     |          0.491 |        0.305 |        0.790 |
| victim_sexMale | Memphis, TN        |          0.723 |        0.529 |        0.988 |
| victim_sexMale | Miami, FL          |          0.515 |        0.304 |        0.872 |
| victim_sexMale | Milwaukee, wI      |          0.727 |        0.499 |        1.060 |
| victim_sexMale | Minneapolis, MN    |          0.947 |        0.478 |        1.875 |
| victim_sexMale | Nashville, TN      |          1.018 |        0.672 |        1.542 |
| victim_sexMale | New Orleans, LA    |          0.585 |        0.422 |        0.811 |
| victim_sexMale | New York, NY       |          0.266 |        0.140 |        0.506 |
| victim_sexMale | Oakland, CA        |          0.563 |        0.365 |        0.868 |
| victim_sexMale | Oklahoma City, OK  |          0.974 |        0.624 |        1.520 |
| victim_sexMale | Omaha, NE          |          0.382 |        0.203 |        0.721 |
| victim_sexMale | Philadelphia, PA   |          0.490 |        0.373 |        0.645 |
| victim_sexMale | Pittsburgh, PA     |          0.431 |        0.265 |        0.700 |
| victim_sexMale | Richmond, VA       |          1.006 |        0.498 |        2.033 |
| victim_sexMale | San Antonio, TX    |          0.705 |        0.398 |        1.249 |
| victim_sexMale | Sacramento, CA     |          0.663 |        0.332 |        1.326 |
| victim_sexMale | Savannah, GA       |          0.867 |        0.422 |        1.780 |
| victim_sexMale | San Bernardino, CA |          0.500 |        0.171 |        1.462 |
| victim_sexMale | San Diego, CA      |          0.423 |        0.204 |        0.875 |
| victim_sexMale | San Francisco, CA  |          0.608 |        0.317 |        1.165 |
| victim_sexMale | St. Louis, MO      |          0.696 |        0.525 |        0.924 |
| victim_sexMale | Stockton, CA       |          1.352 |        0.621 |        2.942 |
| victim_sexMale | Tampa, FL          |          0.808 |        0.348 |        1.876 |
| victim_sexMale | Tulsa, OK          |          0.976 |        0.614 |        1.552 |
| victim_sexMale | Washington, DC     |          0.690 |        0.468 |        1.017 |

The dataframe with estimated odds ratios and confidence intervals for
each city is shown above.
